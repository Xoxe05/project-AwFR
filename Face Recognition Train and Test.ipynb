{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Deep Learning Face Recogntion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our Keras imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import os, os.path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS FROM FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2948 images belonging to 5 classes.\n",
      "Found 996 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 16\n",
    "\n",
    "train_data_dir = 'C:/Users/Predator/PycharmProjects/AwFR/Face Recognition/faces/train'\n",
    "validation_data_dir = 'C:/Users/Predator/PycharmProjects/AwFR/Face Recognition/faces/validation'\n",
    "\n",
    "#data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "train_samples = sum(len(files) for a, b, files in os.walk(r'C:/Users/Predator/PycharmProjects/AwFR/Face Recognition/faces/train'))\n",
    "\n",
    "validation_samples = sum(len(files) for a, b, files in os.walk(r'C:/Users/Predator/PycharmProjects/AwFR/Face Recognition/faces/validation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Chandler', 1: 'DIKESH', 2: 'Joey', 3: 'Pheobe', 4: 'Rachel'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = train_generator.class_indices\n",
    "class_labels = {i: j for j, i in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "num_classes = len(class_labels)\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a simple VGG based model for Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,613\n",
      "Trainable params: 1,326,437\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-c5799a808366>:29: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 1.2346 - accuracy: 0.5392\n",
      "Epoch 00001: val_loss improved from inf to 2.23563, saving model to C:/Users/Predator/PycharmProjects/AwFR/Trained models\\face_recognition_friends_vgg.h5\n",
      "184/184 [==============================] - 47s 255ms/step - loss: 1.2346 - accuracy: 0.5392 - val_loss: 2.2356 - val_accuracy: 0.4325\n",
      "Epoch 2/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.7493 - accuracy: 0.7176\n",
      "Epoch 00002: val_loss improved from 2.23563 to 0.88990, saving model to C:/Users/Predator/PycharmProjects/AwFR/Trained models\\face_recognition_friends_vgg.h5\n",
      "184/184 [==============================] - 48s 258ms/step - loss: 0.7493 - accuracy: 0.7176 - val_loss: 0.8899 - val_accuracy: 0.7933\n",
      "Epoch 3/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.6711 - accuracy: 0.7544\n",
      "Epoch 00003: val_loss did not improve from 0.88990\n",
      "184/184 [==============================] - 46s 252ms/step - loss: 0.6711 - accuracy: 0.7544 - val_loss: 1.3516 - val_accuracy: 0.6129\n",
      "Epoch 4/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.8018\n",
      "Epoch 00004: val_loss improved from 0.88990 to 0.60444, saving model to C:/Users/Predator/PycharmProjects/AwFR/Trained models\\face_recognition_friends_vgg.h5\n",
      "184/184 [==============================] - 49s 265ms/step - loss: 0.5630 - accuracy: 0.8018 - val_loss: 0.6044 - val_accuracy: 0.7329\n",
      "Epoch 5/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.5736 - accuracy: 0.8015\n",
      "Epoch 00005: val_loss did not improve from 0.60444\n",
      "184/184 [==============================] - 47s 255ms/step - loss: 0.5736 - accuracy: 0.8015 - val_loss: 2.1536 - val_accuracy: 0.5292\n",
      "Epoch 6/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.4896 - accuracy: 0.8329\n",
      "Epoch 00006: val_loss did not improve from 0.60444\n",
      "184/184 [==============================] - 48s 262ms/step - loss: 0.4896 - accuracy: 0.8329 - val_loss: 9.7462 - val_accuracy: 0.0524\n",
      "Epoch 7/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.8636\n",
      "Epoch 00007: val_loss did not improve from 0.60444\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "184/184 [==============================] - 47s 254ms/step - loss: 0.4207 - accuracy: 0.8636 - val_loss: 1.0686 - val_accuracy: 0.6794\n",
      "Epoch 8/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.8844\n",
      "Epoch 00008: val_loss improved from 0.60444 to 0.05615, saving model to C:/Users/Predator/PycharmProjects/AwFR/Trained models\\face_recognition_friends_vgg.h5\n",
      "184/184 [==============================] - 51s 275ms/step - loss: 0.3493 - accuracy: 0.8844 - val_loss: 0.0562 - val_accuracy: 0.9950\n",
      "Epoch 9/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.9038\n",
      "Epoch 00009: val_loss did not improve from 0.05615\n",
      "184/184 [==============================] - 49s 264ms/step - loss: 0.3065 - accuracy: 0.9038 - val_loss: 0.0717 - val_accuracy: 0.9940\n",
      "Epoch 10/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9192\n",
      "Epoch 00010: val_loss improved from 0.05615 to 0.04854, saving model to C:/Users/Predator/PycharmProjects/AwFR/Trained models\\face_recognition_friends_vgg.h5\n",
      "184/184 [==============================] - 46s 251ms/step - loss: 0.2640 - accuracy: 0.9192 - val_loss: 0.0485 - val_accuracy: 0.9919\n",
      "Epoch 11/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.9151\n",
      "Epoch 00011: val_loss improved from 0.04854 to 0.04702, saving model to C:/Users/Predator/PycharmProjects/AwFR/Trained models\\face_recognition_friends_vgg.h5\n",
      "184/184 [==============================] - 48s 259ms/step - loss: 0.2666 - accuracy: 0.9151 - val_loss: 0.0470 - val_accuracy: 0.9919\n",
      "Epoch 12/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.9267\n",
      "Epoch 00012: val_loss did not improve from 0.04702\n",
      "184/184 [==============================] - 47s 258ms/step - loss: 0.2334 - accuracy: 0.9267 - val_loss: 0.1315 - val_accuracy: 0.9667\n",
      "Epoch 13/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.9256\n",
      "Epoch 00013: val_loss did not improve from 0.04702\n",
      "184/184 [==============================] - 47s 253ms/step - loss: 0.2500 - accuracy: 0.9256 - val_loss: 0.0511 - val_accuracy: 0.9940\n",
      "Epoch 14/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9376\n",
      "Epoch 00014: val_loss did not improve from 0.04702\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "184/184 [==============================] - 47s 255ms/step - loss: 0.2102 - accuracy: 0.9376 - val_loss: 0.8265 - val_accuracy: 0.7107\n",
      "Epoch 15/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9349\n",
      "Epoch 00015: val_loss improved from 0.04702 to 0.03682, saving model to C:/Users/Predator/PycharmProjects/AwFR/Trained models\\face_recognition_friends_vgg.h5\n",
      "184/184 [==============================] - 46s 248ms/step - loss: 0.2013 - accuracy: 0.9349 - val_loss: 0.0368 - val_accuracy: 0.9950\n",
      "Epoch 16/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.9482\n",
      "Epoch 00016: val_loss did not improve from 0.03682\n",
      "184/184 [==============================] - 47s 257ms/step - loss: 0.1813 - accuracy: 0.9482 - val_loss: 0.0383 - val_accuracy: 0.9950\n",
      "Epoch 17/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9553\n",
      "Epoch 00017: val_loss did not improve from 0.03682\n",
      "184/184 [==============================] - 48s 259ms/step - loss: 0.1611 - accuracy: 0.9553 - val_loss: 0.0372 - val_accuracy: 0.9950\n",
      "Epoch 18/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9574\n",
      "Epoch 00018: val_loss did not improve from 0.03682\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "184/184 [==============================] - 43s 234ms/step - loss: 0.1483 - accuracy: 0.9574 - val_loss: 0.0402 - val_accuracy: 0.9950\n",
      "Epoch 19/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.9536\n",
      "Epoch 00019: val_loss did not improve from 0.03682\n",
      "184/184 [==============================] - 43s 232ms/step - loss: 0.1604 - accuracy: 0.9536 - val_loss: 0.0426 - val_accuracy: 0.9950\n",
      "Epoch 20/50\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.9604Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03682\n",
      "184/184 [==============================] - 43s 236ms/step - loss: 0.1409 - accuracy: 0.9604 - val_loss: 0.0441 - val_accuracy: 0.9950\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"C:/Users/Predator/PycharmProjects/AwFR/Trained models/face_recognition_friends_vgg.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.01),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model.save(\"C:/Users/Predator/PycharmProjects/AwFR/face_recognition_friends_vgg.h5\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('C:/Users/Predator/PycharmProjects/AwFR/face_recognition_friends_vgg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model on some real video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib \n",
    "\n",
    "\n",
    "face_classes = class_labels\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=0.8, thickness=1):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
    "    \n",
    "margin = 0.2\n",
    "# load model and weights\n",
    "img_size = 64\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "cap = cv2.VideoCapture('C:/Users/Predator/PycharmProjects/AwFR/Face Recognition/DIKU.mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
    "    preprocessed_faces = []           \n",
    " \n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w, _ = np.shape(input_img)\n",
    "    detected = detector(frame, 1)\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    \n",
    "    preprocessed_faces_emo = []\n",
    "    if len(detected) > 0:\n",
    "        for i, d in enumerate(detected):\n",
    "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "            xw1 = max(int(x1 - margin * w), 0)\n",
    "            yw1 = max(int(y1 - margin * h), 0)\n",
    "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
    "            #faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "            face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
    "            face = cv2.resize(face, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "            face = face.astype(\"float\") / 255.0\n",
    "            face = img_to_array(face)\n",
    "            face = np.expand_dims(face, axis=0)\n",
    "            preprocessed_faces.append(face)\n",
    " \n",
    "        face_labels = []\n",
    "        for i, d in enumerate(detected):\n",
    "            preds = classifier.predict(preprocessed_faces[i])[0]\n",
    "            face_labels.append(face_classes[preds.argmax()])\n",
    "        \n",
    "        # draw results\n",
    "        for i, d in enumerate(detected):\n",
    "            label = \"{}\".format(face_labels[i])\n",
    "            draw_label(frame, (d.left(), d.top()), label)\n",
    "\n",
    "    cv2.imshow(\"Student identified\", frame)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
